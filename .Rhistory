blogdown::serve_site()
(37*(6.5135)+33*(6.5135+1.8198)+35*(6.5135+4.2008))/105
# Optional header image (relative to `static/media/` folder).
#header:
#  caption: ""
#  image: "research.jpg"
---
- 4/2021 Xinjun Wang received Biostatistics Doctoral Award in the Annual GSPH Dean’s Day
- 4/2021 Ying Ding received 2021 James L. Craig Excellence in Education Award from the Graduate School of Public Health (GSPH)
- 3/2021 Xinjun Wang was honored with the Student of the Year Award by the American Statistical Association (ASA) Pittsburgh Chapter
- 3/2021 Wei Chen joined the editorial board of Genome Biology
- 3/2021 Xinjun Wang won the Outstanding Student Research Award in the Annual Biostatistics Research Day
- 3/2021 Molin Yue won the Best Oral Presentations Award in the Annual Biostatistics Research Day
- 3//2021 Zhongli Xu received an Abstract Scholarship from Assembly on Allergy, Immunology and Inflammation (AII) in ATS
- 3/2021 Zhongli Xu’s abstract was accepted for oral presentation at 2021 ATS International Conference
- 3/2021 Molin Yue’s abstract was accepted for poster presentation at 2021 ATS International Conference
-	10/2020 Wei Chen received PinCH award to study COVID using mobile health technology
-	9/2020 Xinjun Wang awarded International Chinese Statistical Association (ICSA) Student Paper Award
-	8/2020 Wei Chen received R21 for the analysis of omics data in Hispanic asthma kids
-	7/2020 Wei Chen received R21 for the analysis of imaging and genetic data for age-related macular degeneration
-	6/2020 Xinjun Wang received RAC Graduate Student Fellowship from UPMC Children’s Hospital of Pittsburgh
-	6/2020 Rong Zhang defended his dissertation on High-Dimensional Inference of Modified Poisson-Type Graphical Models and Robust Sparse CCA, with Applications to Large-Scale Omics Data
-	4/2020 Tao Sun defended his dissertation on NEW STATISTICAL METHODS FOR COMPLEX SURVIVAL DATA WITH HIGH-DIMENSIONAL COVARIATES
-	4/2020 Tao Sun was inducted to the Delta Omega Honor Society, which is a renowed national honorary society (founded in 1924) in the field of public health
-	3/2020 Zhe Sun was nominated Outstanding Student Award from Graduate School of Public Health (GSPH)
-	3/2020 Yue Wei won the Honorable Mentions for the oral presentation in the Annual Biostatistics Research Day
-	3/2020 Tao Sun won the Best Presentation Award in the Annual Biostatistics Research Day
-	11/2019 Tao Sun received a Travel Award at the MD Anderson Cancer Center to attend the Integrative Biostatistics Research for Imaging, Genomics and High-throughput Technologies in Precision Medicine (iBRIGHT) conference
-	7/2019 Xinjun Wang, with Dr. Wei Chen, awarded CTSI QuMP Pilots Funding, an NIH grant, from University of Pittsburgh
-	6/2019 Tao Sun received a Travel Award for the Deep Learning Workshop hosted by the Statistical and Applied Mathematical Sciences Institute (SAMSI)
-	5/2019 Tao Sun received the Lifetime Data Science Conference (LiDS) 2019 Best Poster Award
-	4/2019 Tao Sun received the Outstanding Teaching Assistant Award for being a primary instructor in the Department of Biostatistics during both 2017 and 2018 academic years
-	4/2019 Tao Sun awarded International Chinese Statistical Association (ICSA) Student Paper Award
-	4/2019 Tao Sun was honored with the Student of the Year Award by the American Statistical Association (ASA) Pittsburgh Chapter
-	4/2019 Yue Wei awarded American Statistical Association (ASA) Pittsburgh Chapter Poster Award
-	4/2019 Rong Zhang awarded American Statistical Association (ASA) Pittsburgh Chapter Student of the Year
-	4/2019 Yue Wei awarded Lifetime Data Science Conference (LiDS) Student Paper Award
-	3/2019 Rong Zhang is one of the five poster winners at ENAR conference
-	1/2019 Ge Yang's abstract was accepted as an oral presentation and Yale Jiang's abstract was accepted as RaPID discussion
-	12/2018 Tao Sun and Zhe Sun both received ENAR Distinguish Student Paper Award, which is one of the most prestigious student awards in the field of Biostatistics
-	11/2018 Tao Sun received CTSI Young Investigator Award for developing novel deep learning methodologies and applications from the University of Pittsburgh
(37*(6.5135)+33*(6.5135+1.8198)+35*(6.5135+4.2008))/105
37*(6.5135)
33*(6.5135+1.8198)
35*(6.5135+4.2008)
a = 6.5145
b = 6.5135+1.8198
a = 6.5135
c = 6.5135+4.2008
(2a-b-c)^2
(2a-b-c)^2
(2*a-b-c)^2
(2*a-b-c)^2/(4/37+1/33+1/35)
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
e
e^1
1-exp(0.2(1-exp(1.5)))
exp(1.5)
1-exp(1.5)
0.2(1-exp(1.5))
0.2*(1-exp(1.5))
1-exp(0.2*(1-exp(1.5)))
### Calculating Wald and Score Confidence Interval with Simulation ##
set.seed(123456)
N <- 100000
pie <- 0.9 ## pi is already defined in R thus we use "pie"
n <- 30
alph <- 0.05 # For 95% Confidence Interval
y <- rbinom(N, n, pie)
pihat <- y/n
## Wald Confidence Interval ##
L.wald <- pihat - qnorm(1-alph/2)*sqrt((pihat*(1-pihat))/n)
U.wald <- pihat + qnorm(1-alph/2)*sqrt((pihat*(1-pihat))/n)
pi_in_wald_CI <- (pie > L.wald)*(pie < U.wald)
observedConfLevel_WaldCI <- mean(pi_in_wald_CI)
observedConfLevel_WaldCI
## Scores Confidence Interval ##
nprime <- n + (qnorm(1-alph/2))^2
w1 <- n/nprime
w2 <- ((qnorm(1-alph/2))^2)/nprime
midpoint <- pihat*w1+0.5*w2
L.score <- midpoint - qnorm(1-alph/2)*sqrt((1/nprime)*(pihat*(1-pihat)*w1+0.25*w2))
U.score <- midpoint + qnorm(1-alph/2)*sqrt((1/nprime)*(pihat*(1-pihat)*w1+0.25*w2))
pi_in_score_CI <- (pie >  L.score)*(pie <  U.score)
observedConfLevel_scoreCI <- mean(pi_in_score_CI)
observedConfLevel_scoreCI
#R code for Likelihood Ratio based Confidence interval
# p 12 Aggresti
library(rootSolve)
install.packages("rootSolve")
#R code for Likelihood Ratio based Confidence interval
# p 12 Aggresti
library(rootSolve)
n <- 30
y <- 27
phat <- y/n
alpha <- 0.05
f1 <- function(pi0) {
-2*(y*log(pi0) + (n-y)*log(1-pi0)-y*log(phat)
-(n-y)*log(1-phat)) - qchisq(1-alpha,df=1)
}
uniroot.all(f=f1, interval=c(0.000001,0.999999))
pdf("uniroot.pdf")
par(oma = c(1,1,0,1), bty = "n")
curve(f1, from=0, to=1, xlab=expression(pi[0]),
ylab=expression(paste("-2log(" ) ~ Lambda ~ paste( ") - ") ~ chi[1]^2 ~
paste( "(" )   ~ alpha ~ paste( ")" ) ))
abline(h=0, col="red")
dev.off()
pdf("uniroot.pdf")
par(oma = c(1,1,0,1), bty = "n")
curve(f1, from=0, to=1, xlab=expression(pi[0]),
ylab=expression(paste("-2log(" ) ~ Lambda ~ paste( ") - ") ~ chi[1]^2 ~
paste( "(" )   ~ alpha ~ paste( ")" ) ))
abline(h=0, col="red")
dev.off()
#R code for finding the MLE of pi where Y~Bin(10, pi)
# and obsered y = 3
pdf("lik.pdf", height = 7, width = 10)
par(mfrow = c(1,2))
## Using log-loikelihood ##
log_likelihood <- function(pi) { 3*log(pi) + 7*log(1-pi) }
curve(log_likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(L(pi)))
opt.log_lik <- optimize(log_likelihood, interval=c(0, 1), maximum=TRUE)
abline(v = opt.log_lik$maximum)
text(opt.log_lik$maximum + 0.05, -15, label = round(opt.lik$maximum, 4))
dev.off()
pdf("lik.pdf", height = 7, width = 10)
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^3)*((1-pi)^7) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
text(opt.lik$maximum + 0.05, 0.0014, label = round(opt.lik$maximum, 4))
## Using log-loikelihood ##
log_likelihood <- function(pi) { 3*log(pi) + 7*log(1-pi) }
curve(log_likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(L(pi)))
opt.log_lik <- optimize(log_likelihood, interval=c(0, 1), maximum=TRUE)
abline(v = opt.log_lik$maximum)
text(opt.log_lik$maximum + 0.05, -15, label = round(opt.lik$maximum, 4))
dev.off()
pdf("lik.pdf", height = 7, width = 10)
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^3)*((1-pi)^7) }
#opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
text(opt.lik$maximum + 0.05, 0.0014, label = round(opt.lik$maximum, 4))
## Using log-loikelihood ##
log_likelihood <- function(pi) { 3*log(pi) + 7*log(1-pi) }
curve(log_likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(L(pi)))
opt.log_lik <- optimize(log_likelihood, interval=c(0, 1), maximum=TRUE)
abline(v = opt.log_lik$maximum)
text(opt.log_lik$maximum + 0.05, -15, label = round(opt.lik$maximum, 4))
dev.off()
pdf("lik.pdf", height = 7, width = 10)
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^3)*((1-pi)^7) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
#text(opt.lik$maximum + 0.05, 0.0014, label = round(opt.lik$maximum, 4))
## Using log-loikelihood ##
log_likelihood <- function(pi) { 3*log(pi) + 7*log(1-pi) }
curve(log_likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(L(pi)))
opt.log_lik <- optimize(log_likelihood, interval=c(0, 1), maximum=TRUE)
abline(v = opt.log_lik$maximum)
text(opt.log_lik$maximum + 0.05, -15, label = round(opt.lik$maximum, 4))
dev.off()
?text
## Using Likelihood ##
likelihood <- function(pi) { (pi^3)*((1-pi)^7) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
opt.lik
round(opt.lik$maximum, 4)
?round
##2.1
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^27)*((1-pi)^3) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
text(opt.lik$maximum + 0.05, 0.0014, label = round(opt.lik$maximum, 4))
## Using log-loikelihood ##
log_likelihood <- function(pi) { 27*log(pi) + 3*log(1-pi) }
curve(log_likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(L(pi)))
opt.log_lik <- optimize(log_likelihood, interval=c(0, 1), maximum=TRUE)
abline(v = opt.log_lik$maximum)
text(opt.log_lik$maximum + 0.05, -15, label = round(opt.lik$maximum, 4))
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^27)*((1-pi)^3) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
text(opt.lik$maximum + 0.05, 1e-05, label = round(opt.lik$maximum, 4))
## Using log-loikelihood ##
log_likelihood <- function(pi) { 27*log(pi) + 3*log(1-pi) }
curve(log_likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(L(pi)))
opt.log_lik <- optimize(log_likelihood, interval=c(0, 1), maximum=TRUE)
abline(v = opt.log_lik$maximum)
text(opt.log_lik$maximum + 0.05, -60, label = round(opt.lik$maximum, 4))
f1(0.5)
rm(list=ls())
clear
### Calculating Wald and Score Confidence Interval with Simulation ##
set.seed(123456)
N <- 100000
pie <- 0.9 ## pi is already defined in R thus we use "pie"
n <- 30
alph <- 0.05 # For 95% Confidence Interval
y <- rbinom(N, n, pie)
pihat <- y/n
## Wald Confidence Interval ##
L.wald <- pihat - qnorm(1-alph/2)*sqrt((pihat*(1-pihat))/n)
U.wald <- pihat + qnorm(1-alph/2)*sqrt((pihat*(1-pihat))/n)
pi_in_wald_CI <- (pie > L.wald)*(pie < U.wald)
observedConfLevel_WaldCI <- mean(pi_in_wald_CI)
observedConfLevel_WaldCI
## Scores Confidence Interval ##
nprime <- n + (qnorm(1-alph/2))^2
w1 <- n/nprime
w2 <- ((qnorm(1-alph/2))^2)/nprime
midpoint <- pihat*w1+0.5*w2
L.score <- midpoint - qnorm(1-alph/2)*sqrt((1/nprime)*(pihat*(1-pihat)*w1+0.25*w2))
U.score <- midpoint + qnorm(1-alph/2)*sqrt((1/nprime)*(pihat*(1-pihat)*w1+0.25*w2))
pi_in_score_CI <- (pie >  L.score)*(pie <  U.score)
observedConfLevel_scoreCI <- mean(pi_in_score_CI)
observedConfLevel_scoreCI
##2.1
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^25)*((1-pi)^5) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^25)*((1-pi)^5) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
text(opt.lik$maximum + 0.05, 1e-05, label = round(opt.lik$maximum, 4))
##2.1
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^25)*((1-pi)^5) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
opt.lik
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
text(opt.lik$maximum + 0.05, 1e-05, label = round(opt.lik$maximum, 4))
text(opt.lik$maximum + 0.05, 1e-06, label = round(opt.lik$maximum, 4))
text(opt.lik$maximum + 0.1, 1e-06, label = round(opt.lik$maximum, 4))
par(mfrow = c(1,2))
## Using Likelihood ##
likelihood <- function(pi) { (pi^25)*((1-pi)^5) }
opt.lik <- optimize(likelihood, interval=c(0, 1), maximum=TRUE)
curve(likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(l(pi)))
abline(v = opt.lik$maximum)
text(opt.lik$maximum + 0.1, 1e-06, label = round(opt.lik$maximum, 4))
## Using log-loikelihood ##
log_likelihood <- function(pi) { 27*log(pi) + 3*log(1-pi) }
curve(log_likelihood, from=0, to=1, xlab=expression(pi), ylab=expression(L(pi)))
opt.log_lik <- optimize(log_likelihood, interval=c(0, 1), maximum=TRUE)
abline(v = opt.log_lik$maximum)
text(opt.log_lik$maximum + 0.05, -60, label = round(opt.lik$maximum, 4))
#R code for Likelihood Ratio based Confidence interval
# p 12 Aggresti
library(rootSolve)
n <- 30
y <- 27
phat <- y/n
alpha <- 0.05
f1 <- function(pi0) {
-2*(y*log(pi0) + (n-y)*log(1-pi0)-y*log(phat)
-(n-y)*log(1-phat)) - qchisq(1-alpha,df=1)
}  ## -2*(y*log(pi0) + (n-y)*log(1-pi0)-y*log(phat)-(n-y)*log(1-phat)) is LRT statistic
f1(0.5)
### 2.4
uniroot.all(f=f1, interval=c(0.000001,0.999999))
par(oma = c(1,1,0,1), bty = "n")
curve(f1, from=0, to=1, xlab=expression(pi[0]),
ylab=expression(paste("-2log(" ) ~ Lambda ~ paste( ") - ") ~ chi[1]^2 ~
paste( "(" )   ~ alpha ~ paste( ")" ) ))
abline(h=0, col="red")
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
